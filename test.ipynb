{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e83f679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from semantic_kernel import Kernel\n",
    "from semantic_kernel.utils.logging import setup_logging\n",
    "from semantic_kernel.functions import kernel_function\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from semantic_kernel.connectors.ai.ollama import OllamaChatCompletion\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.chat_completion_client_base import ChatCompletionClientBase\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.contents import ChatMessageContent, FunctionCallContent, FunctionResultContent\n",
    "\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.azure_chat_prompt_execution_settings import (\n",
    "    AzureChatPromptExecutionSettings,\n",
    ")\n",
    "\n",
    "from semantic_kernel.connectors.mcp import MCPStreamableHttpPlugin\n",
    "\n",
    "import logging\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7b85d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MCP plugin added to kernel\n",
      "ðŸ¤– Assistant ready! You can ask me to use any of the MCP tools listed above.\n",
      "ðŸ’¡ Example: 'Can you help me with fantasy football data?'\n",
      "Type 'exit' to quit.\n",
      "\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 16:02:40 - semantic_kernel.connectors.ai.chat_completion_client_base:284 - INFO] processing 1 tool calls in parallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 16:02:40 - semantic_kernel.kernel:412 - INFO] Calling ff_tools-get_teams function with args: {}\n",
      "[2025-08-20 16:02:40 - semantic_kernel.functions.kernel_function:19 - INFO] Function ff_tools-get_teams invoking.\n",
      "[2025-08-20 16:02:40 - semantic_kernel.functions.kernel_function:29 - INFO] Function ff_tools-get_teams succeeded.\n",
      "[2025-08-20 16:02:40 - semantic_kernel.functions.kernel_function:53 - INFO] Function completed. Duration: 0.011755s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "Here<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "â€™s<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " a<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " quick<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " rundown<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " of<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " all<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " the<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " teams<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " in<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " the<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " league<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " right<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " now<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      ":\n",
      "\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "|<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Team<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Name<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " |<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Short<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " /<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Nick<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "name<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " |<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Team<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " ID<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " |\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "|<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "-----------<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "|<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "----------------<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "--<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "|<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "---------<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "|\n",
      "<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "|<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " America<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      "â€™s<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Fantasy<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Team<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " |<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " Tom<async_generator object ChatCompletionClientBase.get_streaming_chat_message_content at 0x0000015BF43DE5C0>\n",
      " |"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-20 16:02:42 - opentelemetry.context:157 - ERROR] Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\blaineperry\\git\\semantic_kernel_mcp_client\\.venv\\Lib\\site-packages\\opentelemetry\\trace\\__init__.py\", line 589, in use_span\n",
      "    yield span\n",
      "  File \"c:\\Users\\blaineperry\\git\\semantic_kernel_mcp_client\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\ai\\chat_completion_client_base.py\", line 271, in get_streaming_chat_message_contents\n",
      "    yield messages\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\blaineperry\\git\\semantic_kernel_mcp_client\\.venv\\Lib\\site-packages\\opentelemetry\\context\\__init__.py\", line 155, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"c:\\Users\\blaineperry\\git\\semantic_kernel_mcp_client\\.venv\\Lib\\site-packages\\opentelemetry\\context\\contextvars_context.py\", line 53, in detach\n",
      "    self._current_context.reset(token)\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x0000015BF02D3510> at 0x0000015BF41C3100> was created in a different Context\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def main():\n",
    "    # Initialize the kernel\n",
    "    kernel = Kernel()\n",
    "\n",
    "    # Add Azure OpenAI chat completion\n",
    "    chat_completion = OllamaChatCompletion(\n",
    "        ai_model_id=\"gpt-oss:20b\",\n",
    "        host=\"http://ollama.home\",\n",
    "    )\n",
    "    kernel.add_service(chat_completion)\n",
    "\n",
    "    # Set up logging to see detailed information\n",
    "    setup_logging()\n",
    "    \n",
    "    # Configure logging levels for different components\n",
    "    logging.getLogger(\"semantic_kernel\").setLevel(logging.INFO)\n",
    "    logging.getLogger(\"semantic_kernel.kernel\").setLevel(logging.INFO)\n",
    "    logging.getLogger(\"semantic_kernel.connectors\").setLevel(logging.INFO)\n",
    "    \n",
    "    # Set up a basic console handler if not already configured\n",
    "    if not logging.getLogger().handlers:\n",
    "        logging.basicConfig(\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "\n",
    "    \n",
    "    ff_server =  MCPStreamableHttpPlugin(\n",
    "        name=\"ff_tools\",\n",
    "        url=\"http://192.168.86.103:8000/mcp\",\n",
    "    )\n",
    "    await ff_server.connect()  \n",
    "\n",
    "    kernel.add_plugin(ff_server)\n",
    "    print(\"âœ… MCP plugin added to kernel\")\n",
    "\n",
    "    # Enable planning\n",
    "    execution_settings = AzureChatPromptExecutionSettings()\n",
    "    execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()\n",
    "\n",
    "    # Create a history of the conversation\n",
    "    history = ChatHistory()\n",
    "\n",
    "    print(\"ðŸ¤– Assistant ready! You can ask me to use any of the MCP tools listed above.\")\n",
    "    print(\"ðŸ’¡ Example: 'Can you help me with fantasy football data?'\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "    # Initiate a back-and-forth chat\n",
    "    userInput = None\n",
    "    while True:\n",
    "        # Collect user input\n",
    "        userInput = \"what teams are there\"\n",
    "\n",
    "        # Terminate the loop if the user says \"exit\"\n",
    "        if userInput == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Add user input to the history\n",
    "        history.add_user_message(userInput)\n",
    "\n",
    "                # This callback function will be called for each intermediate message,\n",
    "        # which will allow one to handle FunctionCallContent and FunctionResultContent.\n",
    "        # If the callback is not provided, the agent will return the final response\n",
    "        # with no intermediate tool call steps.\n",
    "        async def handle_streaming_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "            for item in message.items or []:\n",
    "                if isinstance(item, FunctionResultContent):\n",
    "                    print(f\"Function Result:> {item.result} for function: {item.name}\")\n",
    "                elif isinstance(item, FunctionCallContent):\n",
    "                    print(f\"Function Call:> {item.name} with arguments: {item.arguments}\")\n",
    "                else:\n",
    "                    print(f\"{item}\")\n",
    "\n",
    "        # Accumulate content so we can add a single message to history at the end\n",
    "        full_response = \"\"\n",
    "\n",
    "        # Enforce streaming-only mode: require invoke_stream on the client\n",
    "        if not hasattr(chat_completion, \"get_streaming_chat_message_contents\"):\n",
    "            raise RuntimeError(\n",
    "                \"The configured chat_completion client does not support streaming (invoke_stream).\\n\"\n",
    "                \"This script is running in streaming-only mode. Use a streaming-capable client.\"\n",
    "            )\n",
    "\n",
    "        thread = None\n",
    "        i = 0\n",
    "        chunks = []\n",
    "        try:\n",
    "            response = chat_completion.get_streaming_chat_message_content(\n",
    "                messages=userInput,\n",
    "                thread=thread,\n",
    "                on_intermediate_message=handle_streaming_intermediate_steps,\n",
    "                chat_history=history,\n",
    "                settings=execution_settings,\n",
    "                kernel=kernel,\n",
    "            )\n",
    "                \n",
    "            async for chunk in response:\n",
    "                chunks.append(chunk)\n",
    "                i+=1\n",
    "                print(chunk, end=\"\")\n",
    "\n",
    "                if i > 90:\n",
    "                    return chunks, ff_server\n",
    "\n",
    "                    break\n",
    "                \n",
    "                print(response)\n",
    "                # thread = response.thread\n",
    "                # if first_chunk:\n",
    "                #     print(f\"# {response.name}: \", end=\"\", flush=True)\n",
    "                #     first_chunk = False\n",
    "                # print(response.content, end=\"\", flush=True)\n",
    "            print()\n",
    "            # Newline after stream finishes\n",
    "            print()\n",
    "\n",
    "            if full_response:\n",
    "                history.add_message(full_response)\n",
    "        finally:\n",
    "            # Clean up the thread on the remote service if provided\n",
    "            if thread:\n",
    "                try:\n",
    "                    await thread.delete()\n",
    "                except Exception:\n",
    "                    # Best-effort cleanup; ignore errors\n",
    "                    pass\n",
    "    await ff_server.close()\n",
    "\n",
    "\n",
    "response, ff_server = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03f7fb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(role='assistant', content='', thinking='User', images=None, tool_name=None, tool_calls=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0].inner_content['message']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8f894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 0:\n",
      "chunk 1:\n",
      "chunk 2:\n",
      "chunk 3:\n",
      "chunk 4:\n",
      "chunk 5:\n",
      "chunk 6:\n",
      "chunk 7:\n",
      "chunk 8:\n",
      "chunk 9:\n",
      "chunk 10:\n",
      "chunk 11:\n",
      "chunk 12:\n",
      "chunk 13:\n",
      "chunk 14:\n",
      "chunk 15:\n",
      "chunk 16:\n",
      "chunk 17:\n",
      "chunk 18:\n",
      "chunk 19:\n",
      "chunk 20:\n",
      "chunk 21:\n",
      "chunk 22:\n",
      "chunk 23:\n",
      "chunk 24:\n",
      "chunk 25:\n",
      "chunk 26:\n",
      "chunk 27:\n",
      "chunk 28:\n",
      "chunk 29:\n",
      "chunk 30:\n",
      "chunk 31:\n",
      "chunk 32:\n",
      "chunk 33:\n",
      "chunk 34:\n",
      "chunk 35:\n",
      "chunk 36:\n",
      "chunk 37:\n",
      "chunk 38:\n",
      "chunk 39:\n",
      "chunk 40:\n",
      "chunk 41:\n",
      "chunk 42:\n",
      "chunk 43:\n",
      "chunk 44:\n",
      "chunk 45:\n",
      "chunk 46:\n",
      "chunk 47:\n",
      "chunk 48:\n",
      "Herechunk 49:\n",
      "â€™schunk 50:\n",
      " achunk 51:\n",
      " quickchunk 52:\n",
      " rundownchunk 53:\n",
      " ofchunk 54:\n",
      " allchunk 55:\n",
      " thechunk 56:\n",
      " teamschunk 57:\n",
      " inchunk 58:\n",
      " thechunk 59:\n",
      " leaguechunk 60:\n",
      " rightchunk 61:\n",
      " nowchunk 62:\n",
      ":\n",
      "\n",
      "chunk 63:\n",
      "|chunk 64:\n",
      " Teamchunk 65:\n",
      " Namechunk 66:\n",
      " |chunk 67:\n",
      " Shortchunk 68:\n",
      " /chunk 69:\n",
      " Nickchunk 70:\n",
      "namechunk 71:\n",
      " |chunk 72:\n",
      " Teamchunk 73:\n",
      " IDchunk 74:\n",
      " |\n",
      "chunk 75:\n",
      "|chunk 76:\n",
      "-----------chunk 77:\n",
      "|chunk 78:\n",
      "----------------chunk 79:\n",
      "--chunk 80:\n",
      "|chunk 81:\n",
      "---------chunk 82:\n",
      "|\n",
      "chunk 83:\n",
      "|chunk 84:\n",
      " Americachunk 85:\n",
      "â€™schunk 86:\n",
      " Fantasychunk 87:\n",
      " Teamchunk 88:\n",
      " |chunk 89:\n",
      " Tomchunk 90:\n",
      " |"
     ]
    }
   ],
   "source": [
    "chunk_num = 0\n",
    "for chunk in response:\n",
    "    print(f\"chunk {chunk_num}:\")\n",
    "    if chunk.inner_content is not None and chunk.inner_content.get('chunk') is not None and chunk.inner_content['chunk'].thinking is not None:\n",
    "        print(chunk.inner_content[\"chunk\"].thinking, end=\"\")\n",
    "\n",
    "    elif chunk.inner_content is not None and chunk.inner_content.get('chunk') is not None and chunk.inner_content['chunk'].tool_calls is not None:\n",
    "        print(chunk.inner_content[\"chunk\"].tool_calls)\n",
    "        tool_calls = chunk.inner_content[\"chunk\"].tool_calls\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        print(chunk, end=\"\")\n",
    "    chunk_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f9336a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MCPStreamableHttpPlugin' object has no attribute 'list_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mff_server\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_tools\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'MCPStreamableHttpPlugin' object has no attribute 'list_tools'"
     ]
    }
   ],
   "source": [
    "ff_server.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401742f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
