{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b51849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['azure_agent', 'Ollama_agent'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from agent import Agent\n",
    "import json\n",
    "\n",
    "# Load agent definition from the JSON file\n",
    "import pathlib\n",
    "\n",
    "agent_def_path = pathlib.Path(\"agent_definition.json\")\n",
    "if not agent_def_path.exists():\n",
    "    raise FileNotFoundError(f\"Agent definition file not found: {agent_def_path.resolve()}\")\n",
    "\n",
    "with agent_def_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    agent_definition = json.load(f)\n",
    "\n",
    "# # Show the loaded definition\n",
    "\n",
    "agent_definition.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296244f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent: Ollama_agent\n",
      "Agent config: {'deployment_name': 'gpt-oss:20b', 'endpoint': 'http://ollama.home', 'system_message': 'you are an expert and help find answers. you have tools, and can use them', 'servers': {'ff_tools': {'url': 'http://192.168.86.103:8000/mcp', 'type': 'http'}, 'sql_tools': {'url': 'https://somesseserver.azurewebsites.us/sse', 'type': 'http'}}, 'inputs': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to connect to MCP server sql_tools: 'NoneType' object has no attribute 'list_tools'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools:  ['get_teams', 'get_roster', 'get_player_data', 'get_waivers', 'update_roster', 'make_waiver_claim', 'analyze_trade_values', 'get_player_rank']\n"
     ]
    }
   ],
   "source": [
    "agent_name = \"Ollama_agent\"\n",
    "print(f\"Creating agent: {agent_name}\")\n",
    "print(f\"Agent config: {agent_definition[agent_name]}\")\n",
    "ff_agent = await Agent.create(agent_definition[agent_name])\n",
    "print(\"Available tools: \", [tool.name for tool in ff_agent.available_tools.tools])\n",
    "result = ff_agent.run_agent(\"What teams are in the league?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82f62de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_name = \"azure_agent\"\n",
    "# print(f\"Creating agent: {agent_name}\")\n",
    "# print(f\"Agent config: {agent_definition[agent_name]}\")\n",
    "# ff_agent = await Agent.create(agent_definition[agent_name])\n",
    "# result = await ff_agent.run_agent(\"Who is active on Blaine's team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b225ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azure_agent', 'Ollama_agent']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "agents = list(agent_definition.keys())\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d2ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:semantic_kernel.connectors.ai.chat_completion_client_base:processing 1 tool calls in parallel.\n",
      "INFO:semantic_kernel.kernel:Calling ff_tools-get_teams function with args: {}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function ff_tools-get_teams invoking.\n",
      "INFO:semantic_kernel.kernel:Calling ff_tools-get_teams function with args: {}\n",
      "INFO:semantic_kernel.functions.kernel_function:Function ff_tools-get_teams invoking.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function ff_tools-get_teams succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.039998s\n",
      "INFO:semantic_kernel.functions.kernel_function:Function ff_tools-get_teams succeeded.\n",
      "INFO:semantic_kernel.functions.kernel_function:Function completed. Duration: 0.039998s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thoughts': 'User asks: \"What teams are in the league?\" Likely need to call ff_tools_get_teams.We need to answer \"What teams are in the league?\" We have the output from get_teams. That gives a dictionary mapping team names to details. We should format the answer: list the team names. Possibly give short names. The user asked \"What teams are in the league?\" So we can say:\\n\\n- America\\'s Fantasy Team\\n- Chance (not Chase)\\n- Gettin\\' Breecey Wit It\\n- GRI... etc.\\n\\nBetter to list all.\\n\\nLet\\'s parse the JSON. It lists keys: America\\'s Fantasy Team, Chance (not Chase), Gettin\\' Breecey Wit It, GRI..., In GPT We Trust, Joe Buck Yourself, Kool name still pending, Mr. Rodgers Naber-hood, Off To Grandma, Penix Pumpers, Redstone Red Rockets, Yinzer Fan Club.\\n\\nThus 12 teams.\\n\\nWe can output that list. Also maybe provide short names. But user just asked for teams. So list names.\\n\\nThus answer.', 'tool_calls': [{'ff_tools-get_teams': {}}], 'messages': 'Here are all the teams currently in the league:\\n\\n1. **America’s Fantasy Team**  \\n2. **Chance (not Chase)**  \\n3. **Gettin’ Breecey Wit It**  \\n4. **Gridiron Gang**  \\n5. **In GPT We Trust**  \\n6. **Joe Buck Yourself**  \\n7. **Kool name still pending**  \\n8. **Mr. Rodgers Naber‑hood**  \\n9. **Off To Grandma**  \\n10. **Penix Pumpers**  \\n11. **Redstone Red Rockets**  \\n12. **Yinzer Fan Club**\\n\\n(There are 12 teams in total.)'}\n"
     ]
    }
   ],
   "source": [
    "async for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d50e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-57' coro=<MCPPluginBase._inner_connect() done, defined at c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\mcp.py:264> exception=KernelPluginInvalidConfigurationError('Failed to connect to the MCP server. Please check your configuration.')>\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\mcp.py\", line 267, in _inner_connect\n",
      "  |     transport = await self._exit_stack.enter_async_context(self.get_mcp_client())\n",
      "  |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\blain\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\contextlib.py\", line 659, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"C:\\Users\\blain\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\contextlib.py\", line 210, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 54, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 101, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 101, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 78, in handle_async_request\n",
      "    |     stream = await self._connect(request)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py\", line 124, in _connect\n",
      "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_backends\\auto.py\", line 31, in connect_tcp\n",
      "    |     return await self._backend.connect_tcp(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py\", line 113, in connect_tcp\n",
      "    |     with map_exceptions(exc_map):\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\blain\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    |     raise to_exc(exc) from exc\n",
      "    | httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\mcp\\client\\sse.py\", line 60, in sse_client\n",
      "    |     async with aconnect_sse(\n",
      "    |                ^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\blain\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx_sse\\_api.py\", line 74, in aconnect_sse\n",
      "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\blain\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1583, in stream\n",
      "    |     response = await self.send(\n",
      "    |                ^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 393, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"C:\\Users\\blain\\AppData\\Roaming\\uv\\python\\cpython-3.12.10-windows-x86_64-none\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 118, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "    +------------------------------------\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\blain\\Documents\\git\\semantic_kernel_mcp\\.venv\\Lib\\site-packages\\semantic_kernel\\connectors\\mcp.py\", line 271, in _inner_connect\n",
      "    raise KernelPluginInvalidConfigurationError(\n",
      "semantic_kernel.exceptions.kernel_exceptions.KernelPluginInvalidConfigurationError: Failed to connect to the MCP server. Please check your configuration.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'async_generator' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mthoughts\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'async_generator' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "result.get(\"thoughts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5653c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get(\"tool_calls\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.get(\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051db4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await ff_agent.run_agent(\"what is in my db??\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
